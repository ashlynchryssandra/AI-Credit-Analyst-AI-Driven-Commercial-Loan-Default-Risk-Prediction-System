# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/199uCsj6MNW0IO2kNX40MrUZ7Z4VXx9x-
"""

# 1. Install and Import Libraries

# A modern data visualization library for professional plots
!pip install --quiet seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files # Used for easy file upload in Colab

# Machine Learning Models and Utilities
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix

# Set a style for better visualization
sns.set_style("whitegrid")
print("Libraries imported successfully!")

# 2. Upload and Load the Data

# This command will open a dialog to upload your file.
# Upload the file: 'combine the files and make it one excel sheet.xlsx - combine the files and make it o.csv'
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

# Load the data into a Pandas DataFrame
df = pd.read_csv(file_name)

# Display the first 5 rows and data information
print(f"File loaded: {file_name}")
print("\n--- DataFrame Head ---")
print(df.head())
print("\n--- Data Information (Columns & Types) ---")
df.info()

# 3. Clean Column Names
# Standardize names for easier access (e.g., remove parentheses and spaces)
df.columns = df.columns.str.lower().str.replace('[^a-z0-9_]+', '', regex=True)
df.rename(columns={'loan_amount_': 'loan_amount', 'interest_rate_': 'interest_rate'}, inplace=True)
df.rename(columns={'ebitda': 'ebitda', 'operating_margin_': 'operating_margin'}, inplace=True)

# 4. Feature Engineering
# a) Calculate Age of Company (Tenure)
df['incorporation_date'] = pd.to_datetime(df['incorporation_date'])
df['report_date'] = pd.to_datetime(df['report_date'])
df['company_age_years'] = (df['report_date'] - df['incorporation_date']).dt.days / 365.25

# b) Calculate Current Ratio (Liquidity)
df['current_ratio'] = df['current_assets'] / df['current_liabilities']

# c) Calculate Debt-to-EBITDA Ratio (Leverage/Repayment Capacity)
df['debt_to_ebitda'] = df['loan_amount'] / df['ebitda'].replace(0, np.nan) # Handle division by zero

# d) Days Since Last Payment (A measure of recent risk)
df['payment_date'] = pd.to_datetime(df['payment_date'])
df['days_since_last_payment'] = (pd.to_datetime('today') - df['payment_date']).dt.days
# For a better assignment, you might use a fixed date or impute for missing payments.

print("\n--- New Engineered Features (Sample) ---")
print(df[['company_age_years', 'current_ratio', 'debt_to_ebitda', 'days_since_last_payment']].head())

# Drop original date/ID columns that are no longer needed
df_cleaned = df.drop(columns=['loan_id', 'customer_id', 'company_name', 'incorporation_date', 'issue_date', 'report_date', 'payment_date'])

# 5. Exploratory Data Analysis (EDA)

# a) Target Variable Distribution (How balanced is the data?)
plt.figure(figsize=(6, 4))
sns.countplot(x='default_flag01', data=df_cleaned)
plt.title('Default Status Distribution')
plt.xlabel('Default Flag (0: Non-Default, 1: Default)')
plt.ylabel('Count')
plt.show()

# b) Default Rate vs. Credit Score
plt.figure(figsize=(10, 6))
sns.histplot(data=df_cleaned, x='credit_score', hue='default_flag01', kde=True, bins=20)
plt.title('Default Status by Credit Score')
plt.xlabel('Credit Score')
plt.show()

# c) Default Rate by GST/Tax Filing Delays (A powerful predictor!)
plt.figure(figsize=(8, 5))
sns.barplot(x='gsttaxfilingdelayspast12m', y='default_flag01', data=df_cleaned)
plt.title('Default Probability vs. GST/Tax Filing Delays')
plt.xlabel('GST/Tax Filing Delays (Past 12 Months)')
plt.ylabel('Default Probability')
plt.show()

print(df_cleaned.columns)

# 6. Preprocessing and Model Setup

# Rename the target column (it was cleaned in Step 2)
df_cleaned.rename(columns={'default_flag01': 'default_flag'}, inplace=True)

# Handle NaN/Inf values created by division (e.g., debt_to_ebitda)
# Impute with the mean or a large number for safety
df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)
df_cleaned.fillna(df_cleaned.mean(numeric_only=True), inplace=True)

# Define Target (y) and Features (X)
X = df_cleaned.drop('default_flag', axis=1)
y = df_cleaned['default_flag']

# Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define features for different preprocessing steps
numerical_features = [
    'loan_amount', 'interest_rate', 'term_months', 'number_of_employees',
    'revenue', 'profit_after_tax', 'ebitda', 'operating_margin',
    'current_assets', 'current_liabilities', 'credit_score',
    'prior_defaults_count', 'prior_loans_count', 'bank_account_transactionscountmonth',
    'gsttaxfilingdelayspast12m', 'digital_payments_volumemonth',
    'company_age_years', 'current_ratio', 'debt_to_ebitda', 'days_since_last_payment'
]

categorical_features = ['registration_type', 'region', 'repayment_schedule', 'industry_codenaics']


# Create Preprocessing Pipelines
# 1. Numerical Pipeline: Impute missing values then Standardize (scale)
numerical_pipeline = Pipeline([
    ('scaler', StandardScaler())
])

# 2. Categorical Pipeline: One-Hot Encode
categorical_pipeline = Pipeline([
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine Pipelines using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ],
    remainder='drop' # Drop any other columns not in the lists
)

# Final Modeling Pipeline: Preprocessor + Random Forest Classifier
model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))
])

# Train the Model
print("\nTraining Random Forest Model...")
model_pipeline.fit(X_train, y_train)
print("Training Complete!")

# 7. Model Evaluation

# Generate predictions and probabilities
y_pred = model_pipeline.predict(X_test)
y_prob = model_pipeline.predict_proba(X_test)[:, 1]

print("\n--- 7a. Classification Report ---")
# Classification Report: Shows Precision, Recall, and F1-Score for each class
print(classification_report(y_test, y_pred))

# Calculate AUC-ROC Score (Area Under the Receiver Operating Characteristic Curve)
# This is the gold standard for binary classification models
roc_auc = roc_auc_score(y_test, y_prob)
print(f"\n--- 7b. AUC-ROC Score: {roc_auc:.4f} ---")


# Plot the Confusion Matrix (Visualizing correct vs. incorrect predictions)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Non-Default (0)', 'Default (1)'],
            yticklabels=['Non-Default (0)', 'Default (1)'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc:.4f})', color='darkorange')
plt.plot([0, 1], [0, 1], 'k--', label='No Skill (AUC = 0.5)')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Optional: Feature Importance
feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()
importances = model_pipeline.named_steps['classifier'].feature_importances_
feature_importance = pd.Series(importances, index=feature_names).sort_values(ascending=False)

plt.figure(figsize=(10, 8))
feature_importance.head(10).plot(kind='barh')
plt.title('Top 10 Most Important Features')
plt.show()

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(7, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="coolwarm", cbar=False, linewidths=1, linecolor='black',
            xticklabels=['Predicted Safe (0)', 'Predicted Default (1)'],
            yticklabels=['Actual Safe (0)', 'Actual Default (1)'])
plt.title('Confusion Matrix: Where the Bank Wins and Loses')
plt.xlabel('Predicted Loan Outcome')
plt.ylabel('Actual Loan Outcome')
plt.show()


# --- 6c. Feature Importance (Why the AI made its decision)
# Get the one-hot encoded feature names
feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()
# Get the importances from the classifier
importances = model_pipeline.named_steps['classifier'].feature_importances_
# Create a series and sort
feature_importance = pd.Series(importances, index=feature_names).sort_values(ascending=False)

plt.figure(figsize=(10, 8))
# Filter for meaningful names and plot the top 12
top_features = feature_importance.head(12)
top_features.index = [f.split('__')[1] if '__' in f else f for f in top_features.index] # Clean names for plot
top_features.plot(kind='barh', color='darkblue')
plt.title('Top 12 Features Driving Loan Risk Prediction (The "Why")')
plt.xlabel('Feature Importance Score')
plt.gca().invert_yaxis()
plt.show()

# --- 6d. ROC Curve (The Sweet Spot)
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AI Analyst (AUC = {roc_auc:.4f})', color='darkorange', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess (AUC = 0.5)')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print("\n\n--- Project Prudence Execution Complete ---")
print("Your AI Credit Analyst is trained and evaluated.")